logger:
    class_name: crlapi.logger.TFLogger
    log_dir: ./tmp_budget
    verbose: False
    cache_size: 10000
    modulo: 1

stream:
    train:
        class_name: crlapi.sl.streams.cifar10.CIFAR10TrainAnytimeStream
        n_megabatches: ${n_megabatches}
        seed: 432
        directory: ${data_dir}
    evaluation:
        class_name: crlapi.sl.streams.cifar10.CIFAR10EvaluationAnytimeStream
        n_megabatches: 1
        seed: 432
        directory: ${data_dir}

clmodel:
    class_name: crlapi.sl.clmodels.transfer_sp_finetune.Finetune 

    cache_dl: True
    training_batch_size: 128
    training_num_workers: 4
    
    validation_proportion: 0.1
    validation_batch_size: 128
    validation_num_workers: 4

    train_replay_proportion: 0.0
    validation_replay_proportion: 0.0

    max_epochs: 100
    device: ${device}

    patience: 25
    patience_delta: 0.001

    grow_every: 1 

    optim:
        class_name: torch.optim.Adam
        lr: 0.001

    model:
        class_name: crlapi.sl.architectures.sp_vgg.SubnetVGG
        n_channels: 16
        grow_n_units: 4

    kornia_augs: 
      - name: RandomCrop
        size: [32, 32]
        padding: 4
        fill: -1.9
      - name: RandomHorizontalFlip
        p:    0.5

evaluation:
    mode: all_tasks
    batch_size: 64
    num_workers: 2
    device: ${device}

device: 'cuda'
data_dir: ~/.avalanche/data/ #/Users/denoyer/workspace/.data

# --- SLURM configuration
time: 1000
gpus: 1
partition: learnfair
n_megabatches: 2

hydra: 
  launcher: 
    nodes: 1
    mem_gb: 64
    max_num_timeout: 3
    cpus_per_task: 10
    signal_delay_s: 30
    timeout_min: ${time}
    gpus_per_node: ${gpus}
    tasks_per_node: ${gpus}
    submitit_folder: '/checkpoint/lucaspc/grow_moe_refac/output'
    partition: ${partition}
  job_logging: 
    root: 
      handlers: []
  #run: 
    #dir: '/private/home/lucaspc/grow_moe_refac/output'

defaults:
  - hydra/launcher: submitit_slurm
